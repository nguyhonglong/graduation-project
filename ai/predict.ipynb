{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import math\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.interpolate import interp1d\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time Series Dataset\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, data, seq_length=30, pred_length=5, augment=True):\n",
    "        self.data = torch.FloatTensor(data)\n",
    "        self.seq_length = seq_length\n",
    "        self.pred_length = pred_length\n",
    "        self.augment = augment\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.seq_length - self.pred_length + 1\n",
    "\n",
    "    def augment_timeseries(self, x):\n",
    "        # Randomly choose augmentation method\n",
    "        aug_type = random.choice(['jitter', 'scaling', 'magnitude_warp', 'none'])\n",
    "        \n",
    "        if aug_type == 'none' or not self.augment:\n",
    "            return x\n",
    "            \n",
    "        if aug_type == 'jitter':\n",
    "            # Add random noise\n",
    "            noise_level = 0.01\n",
    "            noise = torch.randn(x.shape) * noise_level\n",
    "            return x + noise\n",
    "            \n",
    "        elif aug_type == 'scaling':\n",
    "            # Random scaling\n",
    "            scaling_factor = random.uniform(0.95, 1.05)\n",
    "            return x * scaling_factor\n",
    "            \n",
    "        elif aug_type == 'magnitude_warp':\n",
    "            # Magnitude warping\n",
    "            sigma = 0.2\n",
    "            knot = random.randint(3, 5)\n",
    "            orig_steps = np.arange(x.shape[0])\n",
    "            random_warps = np.random.normal(loc=1.0, scale=sigma, size=(knot+2))\n",
    "            warp_steps = (np.linspace(0, x.shape[0]-1., num=knot+2))\n",
    "            warper = interp1d(warp_steps, random_warps, kind='linear')\n",
    "            warper = warper(orig_steps)\n",
    "            return x * torch.FloatTensor(warper.reshape(-1, 1))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data[idx:idx + self.seq_length]\n",
    "        y = self.data[idx + self.seq_length:idx + self.seq_length + self.pred_length]\n",
    "        \n",
    "        if self.augment:\n",
    "            x = self.augment_timeseries(x)\n",
    "        \n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer Time Series Model\n",
    "class TimeSeriesTransformer(nn.Module):\n",
    "    def __init__(self, input_dim, d_model=32, nhead=2, num_layers=1, dropout=0.2):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Simpler embedding\n",
    "        self.embedding = nn.Sequential(\n",
    "            nn.Linear(input_dim, d_model),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "        self.positional_encoding = PositionalEncoding(d_model, dropout)\n",
    "        \n",
    "        # Simpler encoder layer\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=d_model * 2,\n",
    "            dropout=dropout,\n",
    "            activation='relu',\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            encoder_layer,\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "        \n",
    "        # Simpler decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_model, input_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, src, training=True):\n",
    "        # Apply dropout mask during training\n",
    "        x = self.embedding(src)\n",
    "        x = self.positional_encoding(x)\n",
    "        \n",
    "        # Apply attention mask for transformer\n",
    "        mask = self._generate_square_subsequent_mask(src.size(1)) if training else None\n",
    "        x = self.transformer_encoder(x, mask=mask)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "    \n",
    "    def _generate_square_subsequent_mask(self, sz):\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positional Encoding\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.3, max_len=5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.input_dropout = nn.Dropout(p=dropout/2)  # Additional dropout\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.input_dropout(x)  # Apply dropout to input\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Training function\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, weight_decay=0.01):\n",
    "    best_val_loss = float('inf')\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    # Add learning rate scheduler\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, \n",
    "        mode='min', \n",
    "        factor=0.5, \n",
    "        patience=5, \n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        for batch_x, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(batch_x, training=True)\n",
    "            \n",
    "            # Calculate MSE loss\n",
    "            mse_loss = criterion(output[:, -5:, :], batch_y)\n",
    "            \n",
    "            # Add L2 regularization term\n",
    "            l2_reg = torch.tensor(0., requires_grad=True)\n",
    "            for param in model.parameters():\n",
    "                l2_reg = l2_reg + torch.norm(param, p=2)\n",
    "            \n",
    "            # Combined loss with L2 regularization\n",
    "            loss = mse_loss + weight_decay * l2_reg\n",
    "            \n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            total_train_loss += loss.item()\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()  # Disable dropout for validation\n",
    "        val_loss = evaluate_model(model, val_loader, criterion)\n",
    "        \n",
    "        # Update learning rate\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        # Calculate and store losses\n",
    "        avg_train_loss = total_train_loss/len(train_loader)\n",
    "        train_losses.append(avg_train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        # Save best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'train_loss': avg_train_loss,\n",
    "                'val_loss': val_loss,\n",
    "            }, 'best_model.pth')\n",
    "        \n",
    "        print(f'Epoch {epoch+1}, Train Loss: {avg_train_loss:.6f}, Val Loss: {val_loss:.6f}')\n",
    "    \n",
    "    # Plot losses\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(train_losses, label='Training Loss')\n",
    "    plt.plot(val_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Losses')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig('loss_chart.png')\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, val_loader, criterion):\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y in val_loader:\n",
    "            output = model(batch_x)\n",
    "            loss = criterion(output[:, -5:, :], batch_y)\n",
    "            total_val_loss += loss.item()\n",
    "    return total_val_loss / len(val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(data, train_ratio=0.8):\n",
    "    # Normalize the data\n",
    "    scaler = StandardScaler()\n",
    "    normalized_data = scaler.fit_transform(data)\n",
    "    \n",
    "    # Split into train and validation sets\n",
    "    train_size = int(len(normalized_data) * train_ratio)\n",
    "    train_data = normalized_data[:train_size]\n",
    "    val_data = normalized_data[train_size:]\n",
    "    \n",
    "    # Create datasets - only apply augmentation to training data\n",
    "    train_dataset = TimeSeriesDataset(train_data, augment=True)\n",
    "    val_dataset = TimeSeriesDataset(val_data, augment=False)\n",
    "    \n",
    "    # Add dropout to training loader\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "    \n",
    "    return train_loader, val_loader, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load and prepare the data\n",
    "def load_data():\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv('predict.csv')\n",
    "    \n",
    "    # Convert date column to datetime if needed\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "    \n",
    "    # Sort by date to ensure temporal order\n",
    "    df = df.sort_values('Date')\n",
    "    \n",
    "    # Drop the Date column for the model input\n",
    "    features = df.drop('Date', axis=1).values\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_data()\n",
    "    \n",
    "    # Prepare data for training\n",
    "train_loader, val_loader, scaler = prepare_data(data)\n",
    "    \n",
    "    # Initialize model\n",
    "input_dim = 9  # Number of features (Hydrogen, Oxigen, Methane, CO, CO2, Ethylene, Ethane, Acethylene, H2O)\n",
    "model = TimeSeriesTransformer(input_dim=input_dim)\n",
    "    \n",
    "    # Define loss and optimizer with weight decay\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=0.001,\n",
    "    weight_decay=0.01  # L2 regularization coefficient\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nguyl\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 1.631967, Val Loss: 1.108541\n",
      "Epoch 2, Train Loss: 1.458645, Val Loss: 1.046785\n",
      "Epoch 3, Train Loss: 1.392323, Val Loss: 1.021286\n",
      "Epoch 4, Train Loss: 1.346538, Val Loss: 1.017843\n",
      "Epoch 5, Train Loss: 1.317800, Val Loss: 1.012983\n",
      "Epoch 6, Train Loss: 1.302868, Val Loss: 1.007656\n",
      "Epoch 7, Train Loss: 1.267399, Val Loss: 1.007368\n",
      "Epoch 8, Train Loss: 1.299497, Val Loss: 1.009276\n",
      "Epoch 9, Train Loss: 1.259426, Val Loss: 1.007999\n",
      "Epoch 10, Train Loss: 1.240249, Val Loss: 1.005605\n",
      "Epoch 11, Train Loss: 1.249934, Val Loss: 1.006662\n",
      "Epoch 12, Train Loss: 1.195776, Val Loss: 1.007077\n",
      "Epoch 13, Train Loss: 1.197719, Val Loss: 1.005791\n",
      "Epoch 14, Train Loss: 1.180252, Val Loss: 1.005094\n",
      "Epoch 15, Train Loss: 1.159037, Val Loss: 1.005672\n",
      "Epoch 16, Train Loss: 1.145284, Val Loss: 1.004489\n",
      "Epoch 17, Train Loss: 1.137059, Val Loss: 1.003554\n",
      "Epoch 18, Train Loss: 1.142643, Val Loss: 1.004136\n",
      "Epoch 19, Train Loss: 1.130207, Val Loss: 1.002349\n",
      "Epoch 20, Train Loss: 1.123360, Val Loss: 1.003753\n",
      "Epoch 21, Train Loss: 1.104879, Val Loss: 1.003435\n",
      "Epoch 22, Train Loss: 1.101819, Val Loss: 1.002840\n",
      "Epoch 23, Train Loss: 1.107761, Val Loss: 1.001623\n",
      "Epoch 24, Train Loss: 1.081819, Val Loss: 1.001679\n",
      "Epoch 25, Train Loss: 1.080282, Val Loss: 1.001089\n",
      "Epoch 26, Train Loss: 1.063507, Val Loss: 1.000412\n",
      "Epoch 27, Train Loss: 1.063476, Val Loss: 1.000811\n",
      "Epoch 28, Train Loss: 1.075511, Val Loss: 1.001507\n",
      "Epoch 29, Train Loss: 1.050362, Val Loss: 1.001153\n",
      "Epoch 30, Train Loss: 1.042532, Val Loss: 1.001555\n",
      "Epoch 31, Train Loss: 1.039154, Val Loss: 1.001853\n",
      "Epoch 32, Train Loss: 1.030188, Val Loss: 1.002234\n",
      "Epoch 33, Train Loss: 1.042752, Val Loss: 1.002189\n",
      "Epoch 34, Train Loss: 1.032797, Val Loss: 1.001741\n",
      "Epoch 35, Train Loss: 1.016169, Val Loss: 1.001352\n",
      "Epoch 36, Train Loss: 1.024415, Val Loss: 1.001109\n",
      "Epoch 37, Train Loss: 1.016490, Val Loss: 1.000876\n",
      "Epoch 38, Train Loss: 1.026281, Val Loss: 1.000423\n",
      "Epoch 39, Train Loss: 1.016716, Val Loss: 1.000545\n",
      "Epoch 40, Train Loss: 1.014149, Val Loss: 1.000838\n",
      "Epoch 41, Train Loss: 1.013928, Val Loss: 1.000952\n",
      "Epoch 42, Train Loss: 1.009544, Val Loss: 1.001101\n",
      "Epoch 43, Train Loss: 1.004473, Val Loss: 1.001335\n",
      "Epoch 44, Train Loss: 1.033742, Val Loss: 1.001136\n",
      "Epoch 45, Train Loss: 1.013332, Val Loss: 1.000999\n",
      "Epoch 46, Train Loss: 1.009452, Val Loss: 1.000858\n",
      "Epoch 47, Train Loss: 1.018577, Val Loss: 1.000815\n",
      "Epoch 48, Train Loss: 1.000744, Val Loss: 1.000945\n",
      "Epoch 49, Train Loss: 1.006169, Val Loss: 1.001037\n",
      "Epoch 50, Train Loss: 1.006729, Val Loss: 1.001112\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "train_model(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    num_epochs=50,\n",
    "    weight_decay=0.01\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Summary:\n",
      "Input dimension: 9\n",
      "\n",
      "Model Architecture:\n",
      "TimeSeriesTransformer(\n",
      "  (embedding): Sequential(\n",
      "    (0): Linear(in_features=9, out_features=32, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.2, inplace=False)\n",
      "  )\n",
      "  (positional_encoding): PositionalEncoding(\n",
      "    (dropout): Dropout(p=0.2, inplace=False)\n",
      "    (input_dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (transformer_encoder): TransformerEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=32, out_features=64, bias=True)\n",
      "        (dropout): Dropout(p=0.2, inplace=False)\n",
      "        (linear2): Linear(in_features=64, out_features=32, bias=True)\n",
      "        (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.2, inplace=False)\n",
      "        (dropout2): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder): Sequential(\n",
      "    (0): Dropout(p=0.2, inplace=False)\n",
      "    (1): Linear(in_features=32, out_features=9, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "Total parameters: 9,161\n",
      "Trainable parameters: 9,161\n",
      "\n",
      "Layer-wise parameter count:\n",
      "embedding.0.weight: 288\n",
      "embedding.0.bias: 32\n",
      "transformer_encoder.layers.0.self_attn.in_proj_weight: 3,072\n",
      "transformer_encoder.layers.0.self_attn.in_proj_bias: 96\n",
      "transformer_encoder.layers.0.self_attn.out_proj.weight: 1,024\n",
      "transformer_encoder.layers.0.self_attn.out_proj.bias: 32\n",
      "transformer_encoder.layers.0.linear1.weight: 2,048\n",
      "transformer_encoder.layers.0.linear1.bias: 64\n",
      "transformer_encoder.layers.0.linear2.weight: 2,048\n",
      "transformer_encoder.layers.0.linear2.bias: 32\n",
      "transformer_encoder.layers.0.norm1.weight: 32\n",
      "transformer_encoder.layers.0.norm1.bias: 32\n",
      "transformer_encoder.layers.0.norm2.weight: 32\n",
      "transformer_encoder.layers.0.norm2.bias: 32\n",
      "decoder.1.weight: 288\n",
      "decoder.1.bias: 9\n"
     ]
    }
   ],
   "source": [
    "# Print model summary\n",
    "print(\"Model Summary:\")\n",
    "print(f\"Input dimension: {input_dim}\")\n",
    "print(f\"\\nModel Architecture:\")\n",
    "print(model)\n",
    "\n",
    "# Calculate total parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\\nTotal parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "\n",
    "# Print layer-wise parameter count\n",
    "print(\"\\nLayer-wise parameter count:\")\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(f\"{name}: {param.numel():,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to make predictions\n",
    "def predict_next_5_days(model, last_30_days):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "            # Normalize the input\n",
    "        normalized_input = scaler.transform(last_30_days)\n",
    "        input_seq = torch.FloatTensor(normalized_input).unsqueeze(0)\n",
    "            \n",
    "            # Make prediction\n",
    "        prediction = model(input_seq)\n",
    "        prediction = prediction[:, -5:, :]\n",
    "            \n",
    "        # Denormalize the prediction\n",
    "        prediction = scaler.inverse_transform(prediction.squeeze(0))\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicted values for next 5 days:\n",
      "[[ 396.42132127 8918.43259519   59.26813544  254.14850795 1938.97928285\n",
      "   149.29593068   61.68859407  102.95659503   16.69152383]\n",
      " [ 420.41390952 8708.09378417   62.43406352  255.32343642 1943.00510216\n",
      "   157.01369699   64.31529787  104.92422593   16.83537615]\n",
      " [ 366.07454298 8451.72605462   61.46466031  266.67771957 2039.16658207\n",
      "   169.78251072   70.99191564  104.32855051   17.03632605]\n",
      " [ 420.05711405 8710.99682454   62.29891848  255.01339049 1941.8825919\n",
      "   156.51410278   64.10727911  105.24590351   16.83918855]\n",
      " [ 408.26861782 8789.68115218   60.85122148  254.42429368 1939.62074128\n",
      "   152.96090629   62.93980194  104.51081928   16.78263923]]\n"
     ]
    }
   ],
   "source": [
    "# Example of making a prediction\n",
    "# Get the last 30 days from your data\n",
    "last_30_days = data[-30:]\n",
    "predicted_values = predict_next_5_days(model, last_30_days)\n",
    "print(\"\\nPredicted values for next 5 days:\")\n",
    "print(predicted_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Performance Metrics:\n",
      "Training RMSE: 100.0076\n",
      "Training MAE: 43.8019\n"
     ]
    }
   ],
   "source": [
    "# Calculate training and test accuracy\n",
    "def calculate_metrics(model, train_loader, scaler):\n",
    "    model.eval()\n",
    "    train_mse = 0\n",
    "    train_mae = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Training metrics\n",
    "        for X, y in train_loader:\n",
    "            # Forward pass\n",
    "            output = model(X)\n",
    "            output = output[:, -5:, :]  # Get last 5 predictions\n",
    "            \n",
    "            # Denormalize predictions and actual values\n",
    "            output_denorm = scaler.inverse_transform(output.reshape(-1, output.shape[-1]))\n",
    "            y_denorm = scaler.inverse_transform(y.reshape(-1, y.shape[-1]))\n",
    "            \n",
    "            # Calculate metrics\n",
    "            train_mse += torch.mean((torch.tensor(output_denorm) - torch.tensor(y_denorm)) ** 2).item()\n",
    "            train_mae += torch.mean(torch.abs(torch.tensor(output_denorm) - torch.tensor(y_denorm))).item()\n",
    "        \n",
    "        train_mse /= len(train_loader)\n",
    "        train_mae /= len(train_loader)\n",
    "    \n",
    "    return {\n",
    "        'train_rmse': np.sqrt(train_mse),\n",
    "        'train_mae': train_mae\n",
    "    }\n",
    "\n",
    "# Calculate and print metrics\n",
    "metrics = calculate_metrics(model, train_loader, scaler)\n",
    "\n",
    "print(\"\\nModel Performance Metrics:\")\n",
    "print(f\"Training RMSE: {metrics['train_rmse']:.4f}\")\n",
    "print(f\"Training MAE: {metrics['train_mae']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
