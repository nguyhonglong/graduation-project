{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import math\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.interpolate import interp1d\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time Series Dataset\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, data, seq_length=30, pred_length=5, augment=True):\n",
    "        self.data = torch.FloatTensor(data)\n",
    "        self.seq_length = seq_length\n",
    "        self.pred_length = pred_length\n",
    "        self.augment = augment\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.seq_length - self.pred_length + 1\n",
    "\n",
    "    def augment_timeseries(self, x):\n",
    "        # Randomly choose augmentation method\n",
    "        aug_type = random.choice(['jitter', 'scaling', 'magnitude_warp', 'none'])\n",
    "        \n",
    "        if aug_type == 'none' or not self.augment:\n",
    "            return x\n",
    "            \n",
    "        if aug_type == 'jitter':\n",
    "            # Add random noise\n",
    "            noise_level = 0.01\n",
    "            noise = torch.randn(x.shape) * noise_level\n",
    "            return x + noise\n",
    "            \n",
    "        elif aug_type == 'scaling':\n",
    "            # Random scaling\n",
    "            scaling_factor = random.uniform(0.95, 1.05)\n",
    "            return x * scaling_factor\n",
    "            \n",
    "        elif aug_type == 'magnitude_warp':\n",
    "            # Magnitude warping\n",
    "            sigma = 0.2\n",
    "            knot = random.randint(3, 5)\n",
    "            orig_steps = np.arange(x.shape[0])\n",
    "            random_warps = np.random.normal(loc=1.0, scale=sigma, size=(knot+2))\n",
    "            warp_steps = (np.linspace(0, x.shape[0]-1., num=knot+2))\n",
    "            warper = interp1d(warp_steps, random_warps, kind='linear')\n",
    "            warper = warper(orig_steps)\n",
    "            return x * torch.FloatTensor(warper.reshape(-1, 1))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data[idx:idx + self.seq_length]\n",
    "        y = self.data[idx + self.seq_length:idx + self.seq_length + self.pred_length]\n",
    "        \n",
    "        if self.augment:\n",
    "            x = self.augment_timeseries(x)\n",
    "        \n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer Time Series Model\n",
    "class TimeSeriesTransformer(nn.Module):\n",
    "    def __init__(self, input_dim, d_model=32, nhead=2, num_layers=1, dropout=0.2):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Simpler embedding\n",
    "        self.embedding = nn.Sequential(\n",
    "            nn.Linear(input_dim, d_model),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "        self.positional_encoding = PositionalEncoding(d_model, dropout)\n",
    "        \n",
    "        # Simpler encoder layer\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=d_model * 2,\n",
    "            dropout=dropout,\n",
    "            activation='relu',\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            encoder_layer,\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "        \n",
    "        # Simpler decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_model, input_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, src, training=True):\n",
    "        # Apply dropout mask during training\n",
    "        x = self.embedding(src)\n",
    "        x = self.positional_encoding(x)\n",
    "        \n",
    "        # Apply attention mask for transformer\n",
    "        mask = self._generate_square_subsequent_mask(src.size(1)) if training else None\n",
    "        x = self.transformer_encoder(x, mask=mask)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "    \n",
    "    def _generate_square_subsequent_mask(self, sz):\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positional Encoding\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.3, max_len=5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.input_dropout = nn.Dropout(p=dropout/2)  # Additional dropout\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.input_dropout(x)  # Apply dropout to input\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Training function\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, weight_decay=0.01):\n",
    "    best_val_loss = float('inf')\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    # Add learning rate scheduler\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, \n",
    "        mode='min', \n",
    "        factor=0.5, \n",
    "        patience=5, \n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        for batch_x, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(batch_x, training=True)\n",
    "            \n",
    "            # Calculate MSE loss\n",
    "            mse_loss = criterion(output[:, -5:, :], batch_y)\n",
    "            \n",
    "            # Add L2 regularization term\n",
    "            l2_reg = torch.tensor(0., requires_grad=True)\n",
    "            for param in model.parameters():\n",
    "                l2_reg = l2_reg + torch.norm(param, p=2)\n",
    "            \n",
    "            # Combined loss with L2 regularization\n",
    "            loss = mse_loss + weight_decay * l2_reg\n",
    "            \n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            total_train_loss += loss.item()\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()  # Disable dropout for validation\n",
    "        val_loss = evaluate_model(model, val_loader, criterion)\n",
    "        \n",
    "        # Update learning rate\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        # Calculate and store losses\n",
    "        avg_train_loss = total_train_loss/len(train_loader)\n",
    "        train_losses.append(avg_train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        # Save best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'train_loss': avg_train_loss,\n",
    "                'val_loss': val_loss,\n",
    "            }, 'best_model.pth')\n",
    "        \n",
    "        print(f'Epoch {epoch+1}, Train Loss: {avg_train_loss:.6f}, Val Loss: {val_loss:.6f}')\n",
    "    \n",
    "    # Plot losses\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(train_losses, label='Training Loss')\n",
    "    plt.plot(val_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Losses')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig('loss_chart.png')\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, val_loader, criterion):\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y in val_loader:\n",
    "            output = model(batch_x)\n",
    "            loss = criterion(output[:, -5:, :], batch_y)\n",
    "            total_val_loss += loss.item()\n",
    "    return total_val_loss / len(val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(data, train_ratio=0.8):\n",
    "    # Normalize the data\n",
    "    scaler = StandardScaler()\n",
    "    normalized_data = scaler.fit_transform(data)\n",
    "    \n",
    "    # Split into train and validation sets\n",
    "    train_size = int(len(normalized_data) * train_ratio)\n",
    "    train_data = normalized_data[:train_size]\n",
    "    val_data = normalized_data[train_size:]\n",
    "    \n",
    "    # Create datasets - only apply augmentation to training data\n",
    "    train_dataset = TimeSeriesDataset(train_data, augment=True)\n",
    "    val_dataset = TimeSeriesDataset(val_data, augment=False)\n",
    "    \n",
    "    # Add dropout to training loader\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "    \n",
    "    return train_loader, val_loader, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load and prepare the data\n",
    "def load_data():\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv('predict.csv')\n",
    "    \n",
    "    # Convert date column to datetime if needed\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "    \n",
    "    # Sort by date to ensure temporal order\n",
    "    df = df.sort_values('Date')\n",
    "    \n",
    "    # Drop the Date column for the model input\n",
    "    features = df.drop('Date', axis=1).values\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_data()\n",
    "    \n",
    "    # Prepare data for training\n",
    "train_loader, val_loader, scaler = prepare_data(data)\n",
    "    \n",
    "    # Initialize model\n",
    "input_dim = 9  # Number of features (Hydrogen, Oxigen, Methane, CO, CO2, Ethylene, Ethane, Acethylene, H2O)\n",
    "model = TimeSeriesTransformer(input_dim=input_dim)\n",
    "    \n",
    "    # Define loss and optimizer with weight decay\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=0.001,\n",
    "    weight_decay=0.01  # L2 regularization coefficient\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nguyl\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 1.606663, Val Loss: 1.122963\n",
      "Epoch 2, Train Loss: 1.457440, Val Loss: 1.057526\n",
      "Epoch 3, Train Loss: 1.382558, Val Loss: 1.032855\n",
      "Epoch 4, Train Loss: 1.347987, Val Loss: 1.023171\n",
      "Epoch 5, Train Loss: 1.306295, Val Loss: 1.015703\n",
      "Epoch 6, Train Loss: 1.278766, Val Loss: 1.013406\n",
      "Epoch 7, Train Loss: 1.292322, Val Loss: 1.012635\n",
      "Epoch 8, Train Loss: 1.236908, Val Loss: 1.011759\n",
      "Epoch 9, Train Loss: 1.246808, Val Loss: 1.011488\n",
      "Epoch 10, Train Loss: 1.237055, Val Loss: 1.009772\n",
      "Epoch 11, Train Loss: 1.211033, Val Loss: 1.008515\n",
      "Epoch 12, Train Loss: 1.199193, Val Loss: 1.009039\n",
      "Epoch 13, Train Loss: 1.180874, Val Loss: 1.008989\n",
      "Epoch 14, Train Loss: 1.165947, Val Loss: 1.008642\n",
      "Epoch 15, Train Loss: 1.161276, Val Loss: 1.008022\n",
      "Epoch 16, Train Loss: 1.134830, Val Loss: 1.008008\n",
      "Epoch 17, Train Loss: 1.130756, Val Loss: 1.006989\n",
      "Epoch 18, Train Loss: 1.118291, Val Loss: 1.006666\n",
      "Epoch 19, Train Loss: 1.115621, Val Loss: 1.005762\n",
      "Epoch 20, Train Loss: 1.106614, Val Loss: 1.005758\n",
      "Epoch 21, Train Loss: 1.099302, Val Loss: 1.005009\n",
      "Epoch 22, Train Loss: 1.087585, Val Loss: 1.005221\n",
      "Epoch 23, Train Loss: 1.076270, Val Loss: 1.004616\n",
      "Epoch 24, Train Loss: 1.070608, Val Loss: 1.005480\n",
      "Epoch 25, Train Loss: 1.062670, Val Loss: 1.005570\n",
      "Epoch 26, Train Loss: 1.079122, Val Loss: 1.004917\n",
      "Epoch 27, Train Loss: 1.052529, Val Loss: 1.003864\n",
      "Epoch 28, Train Loss: 1.048046, Val Loss: 1.003116\n",
      "Epoch 29, Train Loss: 1.045943, Val Loss: 1.001949\n",
      "Epoch 30, Train Loss: 1.052510, Val Loss: 1.001110\n",
      "Epoch 31, Train Loss: 1.038081, Val Loss: 1.000628\n",
      "Epoch 32, Train Loss: 1.041533, Val Loss: 1.001789\n",
      "Epoch 33, Train Loss: 1.024311, Val Loss: 1.001660\n",
      "Epoch 34, Train Loss: 1.016293, Val Loss: 1.001359\n",
      "Epoch 35, Train Loss: 1.010798, Val Loss: 1.001190\n",
      "Epoch 36, Train Loss: 1.012973, Val Loss: 1.002147\n",
      "Epoch 37, Train Loss: 1.014582, Val Loss: 1.004224\n",
      "Epoch 38, Train Loss: 1.002421, Val Loss: 1.004573\n",
      "Epoch 39, Train Loss: 1.026791, Val Loss: 1.004648\n",
      "Epoch 40, Train Loss: 0.998326, Val Loss: 1.004142\n",
      "Epoch 41, Train Loss: 1.013706, Val Loss: 1.004097\n",
      "Epoch 42, Train Loss: 1.031844, Val Loss: 1.003899\n",
      "Epoch 43, Train Loss: 1.015407, Val Loss: 1.003150\n",
      "Epoch 44, Train Loss: 1.010719, Val Loss: 1.003113\n",
      "Epoch 45, Train Loss: 0.998863, Val Loss: 1.003212\n",
      "Epoch 46, Train Loss: 1.002452, Val Loss: 1.003032\n",
      "Epoch 47, Train Loss: 1.015162, Val Loss: 1.002958\n",
      "Epoch 48, Train Loss: 0.994625, Val Loss: 1.003026\n",
      "Epoch 49, Train Loss: 1.006960, Val Loss: 1.003276\n",
      "Epoch 50, Train Loss: 0.996781, Val Loss: 1.003340\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "train_model(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    num_epochs=50,\n",
    "    weight_decay=0.01\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to make predictions\n",
    "def predict_next_5_days(model, last_30_days):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "            # Normalize the input\n",
    "        normalized_input = scaler.transform(last_30_days)\n",
    "        input_seq = torch.FloatTensor(normalized_input).unsqueeze(0)\n",
    "            \n",
    "            # Make prediction\n",
    "        prediction = model(input_seq)\n",
    "        prediction = prediction[:, -5:, :]\n",
    "            \n",
    "        # Denormalize the prediction\n",
    "        prediction = scaler.inverse_transform(prediction.squeeze(0))\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicted values for next 5 days:\n",
      "[[ 470.62494501 9027.09272807   76.84529919  251.83761115 1887.41177795\n",
      "   184.09427597   69.24358993  128.56298759   15.84264645]\n",
      " [ 473.13205012 8877.91379236   77.94285329  251.47274024 1900.03321579\n",
      "   186.67690604   69.92158928  125.2114669    15.94264956]\n",
      " [ 469.42742114 8716.19210509   79.53857652  252.65086764 1919.48173482\n",
      "   186.35219665   71.32691236  125.14931827   16.01327172]\n",
      " [ 473.09973677 8885.94337981   78.03035123  251.4957296  1898.71266636\n",
      "   186.4726184    69.84251171  125.4007759    15.94566663]\n",
      " [ 471.64796781 8955.47013205   77.59964901  251.67469462 1893.27018805\n",
      "   185.131875     69.55979951  126.93451253   15.90425033]]\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Example of making a prediction\n",
    "# Get the last 30 days from your data\n",
    "last_30_days = data[-30:]\n",
    "predicted_values = predict_next_5_days(model, last_30_days)\n",
    "print(\"\\nPredicted values for next 5 days:\")\n",
    "print(predicted_values)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
