{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import math\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.interpolate import interp1d\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time Series Dataset\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, data, seq_length=30, pred_length=5, augment=True):\n",
    "        self.data = torch.FloatTensor(data)\n",
    "        self.seq_length = seq_length\n",
    "        self.pred_length = pred_length\n",
    "        self.augment = augment\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.seq_length - self.pred_length + 1\n",
    "\n",
    "    def augment_timeseries(self, x):\n",
    "        # Randomly choose augmentation method\n",
    "        aug_type = random.choice(['jitter', 'scaling', 'magnitude_warp', 'none'])\n",
    "        \n",
    "        if aug_type == 'none' or not self.augment:\n",
    "            return x\n",
    "            \n",
    "        if aug_type == 'jitter':\n",
    "            # Add random noise\n",
    "            noise_level = 0.01\n",
    "            noise = torch.randn(x.shape) * noise_level\n",
    "            return x + noise\n",
    "            \n",
    "        elif aug_type == 'scaling':\n",
    "            # Random scaling\n",
    "            scaling_factor = random.uniform(0.95, 1.05)\n",
    "            return x * scaling_factor\n",
    "            \n",
    "        elif aug_type == 'magnitude_warp':\n",
    "            # Magnitude warping\n",
    "            sigma = 0.2\n",
    "            knot = random.randint(3, 5)\n",
    "            orig_steps = np.arange(x.shape[0])\n",
    "            random_warps = np.random.normal(loc=1.0, scale=sigma, size=(knot+2))\n",
    "            warp_steps = (np.linspace(0, x.shape[0]-1., num=knot+2))\n",
    "            warper = interp1d(warp_steps, random_warps, kind='linear')\n",
    "            warper = warper(orig_steps)\n",
    "            return x * torch.FloatTensor(warper.reshape(-1, 1))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data[idx:idx + self.seq_length]\n",
    "        y = self.data[idx + self.seq_length:idx + self.seq_length + self.pred_length]\n",
    "        \n",
    "        if self.augment:\n",
    "            x = self.augment_timeseries(x)\n",
    "        \n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer Time Series Model\n",
    "class TimeSeriesTransformer(nn.Module):\n",
    "    def __init__(self, input_dim, d_model=32, nhead=2, num_layers=1, dropout=0.2):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Simpler embedding\n",
    "        self.embedding = nn.Sequential(\n",
    "            nn.Linear(input_dim, d_model),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "        self.positional_encoding = PositionalEncoding(d_model, dropout)\n",
    "        \n",
    "        # Simpler encoder layer\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=d_model * 2,\n",
    "            dropout=dropout,\n",
    "            activation='relu',\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            encoder_layer,\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "        \n",
    "        # Simpler decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_model, input_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, src, training=True):\n",
    "        # Apply dropout mask during training\n",
    "        x = self.embedding(src)\n",
    "        x = self.positional_encoding(x)\n",
    "        \n",
    "        # Apply attention mask for transformer\n",
    "        mask = self._generate_square_subsequent_mask(src.size(1)) if training else None\n",
    "        x = self.transformer_encoder(x, mask=mask)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "    \n",
    "    def _generate_square_subsequent_mask(self, sz):\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positional Encoding\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.3, max_len=5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.input_dropout = nn.Dropout(p=dropout/2)  # Additional dropout\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.input_dropout(x)  # Apply dropout to input\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Training function\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, weight_decay=0.01):\n",
    "    best_val_loss = float('inf')\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    # Add learning rate scheduler\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, \n",
    "        mode='min', \n",
    "        factor=0.5, \n",
    "        patience=5, \n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        for batch_x, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(batch_x, training=True)\n",
    "            \n",
    "            # Calculate MSE loss\n",
    "            mse_loss = criterion(output[:, -5:, :], batch_y)\n",
    "            \n",
    "            # Add L2 regularization term\n",
    "            l2_reg = torch.tensor(0., requires_grad=True)\n",
    "            for param in model.parameters():\n",
    "                l2_reg = l2_reg + torch.norm(param, p=2)\n",
    "            \n",
    "            # Combined loss with L2 regularization\n",
    "            loss = mse_loss + weight_decay * l2_reg\n",
    "            \n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            total_train_loss += loss.item()\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()  # Disable dropout for validation\n",
    "        val_loss = evaluate_model(model, val_loader, criterion)\n",
    "        \n",
    "        # Update learning rate\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        # Calculate and store losses\n",
    "        avg_train_loss = total_train_loss/len(train_loader)\n",
    "        train_losses.append(avg_train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        # Save best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'train_loss': avg_train_loss,\n",
    "                'val_loss': val_loss,\n",
    "            }, 'best_model.pth')\n",
    "        \n",
    "        print(f'Epoch {epoch+1}, Train Loss: {avg_train_loss:.6f}, Val Loss: {val_loss:.6f}')\n",
    "    \n",
    "    # Plot losses\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(train_losses, label='Training Loss')\n",
    "    plt.plot(val_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Losses')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig('loss_chart.png')\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, val_loader, criterion):\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y in val_loader:\n",
    "            output = model(batch_x)\n",
    "            loss = criterion(output[:, -5:, :], batch_y)\n",
    "            total_val_loss += loss.item()\n",
    "    return total_val_loss / len(val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(data, train_ratio=0.8):\n",
    "    # Normalize the data\n",
    "    scaler = StandardScaler()\n",
    "    normalized_data = scaler.fit_transform(data)\n",
    "    \n",
    "    # Split into train and validation sets\n",
    "    train_size = int(len(normalized_data) * train_ratio)\n",
    "    train_data = normalized_data[:train_size]\n",
    "    val_data = normalized_data[train_size:]\n",
    "    \n",
    "    # Create datasets - only apply augmentation to training data\n",
    "    train_dataset = TimeSeriesDataset(train_data, augment=True)\n",
    "    val_dataset = TimeSeriesDataset(val_data, augment=False)\n",
    "    \n",
    "    # Add dropout to training loader\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "    \n",
    "    return train_loader, val_loader, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load and prepare the data\n",
    "def load_data():\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv('predict.csv')\n",
    "    \n",
    "    # Convert date column to datetime if needed\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "    \n",
    "    # Sort by date to ensure temporal order\n",
    "    df = df.sort_values('Date')\n",
    "    \n",
    "    # Drop the Date column for the model input\n",
    "    features = df.drop('Date', axis=1).values\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_data()\n",
    "    \n",
    "    # Prepare data for training\n",
    "train_loader, val_loader, scaler = prepare_data(data)\n",
    "    \n",
    "    # Initialize model\n",
    "input_dim = 9  # Number of features (Hydrogen, Oxigen, Methane, CO, CO2, Ethylene, Ethane, Acethylene, H2O)\n",
    "model = TimeSeriesTransformer(input_dim=input_dim)\n",
    "    \n",
    "    # Define loss and optimizer with weight decay\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=0.001,\n",
    "    weight_decay=0.01  # L2 regularization coefficient\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nguyl\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 1.550308, Val Loss: 1.081210\n",
      "Epoch 2, Train Loss: 1.440618, Val Loss: 1.034129\n",
      "Epoch 3, Train Loss: 1.383074, Val Loss: 1.018589\n",
      "Epoch 4, Train Loss: 1.332109, Val Loss: 1.011574\n",
      "Epoch 5, Train Loss: 1.306197, Val Loss: 1.008922\n",
      "Epoch 6, Train Loss: 1.278270, Val Loss: 1.006919\n",
      "Epoch 7, Train Loss: 1.306072, Val Loss: 1.010612\n",
      "Epoch 8, Train Loss: 1.241831, Val Loss: 1.008400\n",
      "Epoch 9, Train Loss: 1.236561, Val Loss: 1.007022\n",
      "Epoch 10, Train Loss: 1.225568, Val Loss: 1.010893\n",
      "Epoch 11, Train Loss: 1.199883, Val Loss: 1.007297\n",
      "Epoch 12, Train Loss: 1.197370, Val Loss: 1.006044\n",
      "Epoch 13, Train Loss: 1.170977, Val Loss: 1.006721\n",
      "Epoch 14, Train Loss: 1.164023, Val Loss: 1.006487\n",
      "Epoch 15, Train Loss: 1.180849, Val Loss: 1.005761\n",
      "Epoch 16, Train Loss: 1.153798, Val Loss: 1.005146\n",
      "Epoch 17, Train Loss: 1.125454, Val Loss: 1.006506\n",
      "Epoch 18, Train Loss: 1.121055, Val Loss: 1.005779\n",
      "Epoch 19, Train Loss: 1.114471, Val Loss: 1.005098\n",
      "Epoch 20, Train Loss: 1.092360, Val Loss: 1.005335\n",
      "Epoch 21, Train Loss: 1.097793, Val Loss: 1.005428\n",
      "Epoch 22, Train Loss: 1.098533, Val Loss: 1.005559\n",
      "Epoch 23, Train Loss: 1.076828, Val Loss: 1.005519\n",
      "Epoch 24, Train Loss: 1.086570, Val Loss: 1.005604\n",
      "Epoch 25, Train Loss: 1.072568, Val Loss: 1.005467\n",
      "Epoch 26, Train Loss: 1.078428, Val Loss: 1.004957\n",
      "Epoch 27, Train Loss: 1.060285, Val Loss: 1.004760\n",
      "Epoch 28, Train Loss: 1.074495, Val Loss: 1.004706\n",
      "Epoch 29, Train Loss: 1.054834, Val Loss: 1.004633\n",
      "Epoch 30, Train Loss: 1.062070, Val Loss: 1.004620\n",
      "Epoch 31, Train Loss: 1.064934, Val Loss: 1.003895\n",
      "Epoch 32, Train Loss: 1.047200, Val Loss: 1.003315\n",
      "Epoch 33, Train Loss: 1.056614, Val Loss: 1.003453\n",
      "Epoch 34, Train Loss: 1.040818, Val Loss: 1.003795\n",
      "Epoch 35, Train Loss: 1.051070, Val Loss: 1.003558\n",
      "Epoch 36, Train Loss: 1.048940, Val Loss: 1.002417\n",
      "Epoch 37, Train Loss: 1.034676, Val Loss: 1.001470\n",
      "Epoch 38, Train Loss: 1.030173, Val Loss: 1.001228\n",
      "Epoch 39, Train Loss: 1.035448, Val Loss: 1.001304\n",
      "Epoch 40, Train Loss: 1.081358, Val Loss: 1.001072\n",
      "Epoch 41, Train Loss: 1.015883, Val Loss: 1.000667\n",
      "Epoch 42, Train Loss: 1.027969, Val Loss: 1.000727\n",
      "Epoch 43, Train Loss: 1.011263, Val Loss: 1.001082\n",
      "Epoch 44, Train Loss: 1.041514, Val Loss: 1.001266\n",
      "Epoch 45, Train Loss: 1.021110, Val Loss: 1.000946\n",
      "Epoch 46, Train Loss: 1.011419, Val Loss: 1.000504\n",
      "Epoch 47, Train Loss: 1.009556, Val Loss: 1.000792\n",
      "Epoch 48, Train Loss: 0.999715, Val Loss: 1.001050\n",
      "Epoch 49, Train Loss: 0.998468, Val Loss: 1.001295\n",
      "Epoch 50, Train Loss: 1.007157, Val Loss: 1.001468\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "train_model(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    num_epochs=50,\n",
    "    weight_decay=0.01\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to make predictions\n",
    "def predict_next_5_days(model, last_30_days):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "            # Normalize the input\n",
    "        normalized_input = scaler.transform(last_30_days)\n",
    "        input_seq = torch.FloatTensor(normalized_input).unsqueeze(0)\n",
    "            \n",
    "            # Make prediction\n",
    "        prediction = model(input_seq)\n",
    "        prediction = prediction[:, -5:, :]\n",
    "            \n",
    "        # Denormalize the prediction\n",
    "        prediction = scaler.inverse_transform(prediction.squeeze(0))\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of making a prediction\n",
    "# Get the last 30 days from your data\n",
    "last_30_days = data[-30:]\n",
    "predicted_values = predict_next_5_days(model, last_30_days)\n",
    "print(\"\\nPredicted values for next 5 days:\")\n",
    "print(predicted_values)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
